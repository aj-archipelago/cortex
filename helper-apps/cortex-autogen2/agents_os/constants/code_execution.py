# Code Execution and Validation constants

CODE_EXECUTION_CORE = """
**PRIMARY FUNCTION**: Generate and execute Python code to create files.

**‚ö†Ô∏è CRITICAL: ALWAYS EXECUTE CODE ‚ö†Ô∏è**
- You MUST call `execute_code_bound(code)` after generating Python code
- NEVER just output code blocks without executing them
- Pattern: 1) Generate code ‚Üí 2) Call execute_code_bound(code) ‚Üí 3) Check output
- If you don't call execute_code_bound, your code does NOTHING!

**CRITICAL REQUIREMENT**: Use REAL data from previous agents as described in conversation context. Parse data paths and structures explicitly mentioned by previous agents. NEVER generate synthetic/fallback data.
"""

DATA_QUALITY_REQUIREMENTS = """
**CRITICAL DATA QUALITY REQUIREMENTS**:
- **SOURCE VALIDATION**: Data must come from real sources with proper attribution - never synthetic
- **CONSISTENCY CHECKS**: Verify data completeness, numeric validity, and logical consistency
- **Product/Entity Names**: Use REALISTIC names (e.g., "Headphones", "Laptop", "Coffee Maker"), NEVER generic names like "Product_1", "Item_A"
- **Date Ranges**:
  * For "last 90 days" tasks: generate dates between [ninety_days_ago] and [current_date]
  * For "last 30 days" tasks: generate dates between [thirty_days_ago] and [current_date]
  * For "this year" tasks: use year [current_year]
- **Current Context**: Today is [current_date], use this as your reference for ALL temporal calculations
- **Validate Before Upload**: Check data quality (realistic names, valid dates) before marking files ready for upload
- **File Creation Verification** (UNIVERSAL PRINCIPLE - Applies to ANY file type, ANY task):
  * **PRINCIPLE: VERIFY BEFORE DECLARE**: Verify file exists, is valid, and matches requirements before declaring creation complete
  * **MANDATORY VERIFICATION PATTERN** (use for ANY file creation):
    * Check file exists with `os.path.exists(file_path)`
    * Verify file size > 0 with `os.path.getsize(file_path) > 0`
    * Verify file format is valid (not corrupted, not empty placeholder)
    * Verify file name matches task requirements (if specified in task)
    * Only mark "üìÅ Ready for upload" after ALL verifications pass
  * **Format-Specific Verification** (applies to ANY file format):
    * For JSON files: Validate with `json.load()` to ensure parseable
    * For images: Check file size indicates valid content (typically > 1KB for real images)
    * For PPTX files: Verify slide count > 0 and file size > 50KB (empty PPTX files are ~5-10KB)
    * For PDF files: Verify file size > 10KB (empty PDFs are typically < 5KB)
    * For CSV files: Verify file has content (check row count > 0) - CRITICAL: Do NOT create CSV files with only headers when task expects data
    * For ZIP files: Verify can be opened and contains files
    * For Excel files: Verify can be opened and contains data
    * For any other format: Verify file is not empty and can be opened/read
  * **Empty Data File Prevention** (MANDATORY for ANY data file):
    * Before creating CSV/JSON/data files, validate row count > 0: `if len(df) == 0: raise ValueError("Cannot create file: No data rows available")`
    * If date filtering results in zero records, fail cleanly with clear error message explaining the date range and why no data exists
    * Do NOT create empty files with only headers when task expects actual data
    * Preserve task intent: if task requests "CSV with headlines", ensure it contains actual headline data, not just headers
  * **Requirement Matching Verification**:
    * Extract file requirements from task (names, formats, structure)
    * Verify created files match requirements exactly
    * If mismatch detected ‚Üí adapt file creation to match requirements
    * Only mark complete when requirements met
  * **Error Handling**: If verification fails, print error and retry file creation - DO NOT mark invalid files for upload
  * This principle applies to ANY file creation task, ANY file format, ANY naming requirement
"""

SCOPE_AWARENESS_GUIDANCE = """
**SCOPE AWARENESS**:
- When task specifies exact quantities (e.g., "100 rows", "50 cities"), aim to generate approximately that volume
- For complex calculations (e.g., cities √ó days), calculate the expected total and generate that many records
- Include row count verification when generating data: print(f"Generated {{len(data)}} records")
- Never generate significantly less than requested (e.g., 10 instead of 100), but reasonable approximations are acceptable
"""

VISUALIZATION_GUIDANCE = """
**VISUALIZATION GUIDANCE**:
- Every structured data generation or analysis task MUST output at least **two distinct PNG charts** (trend + comparison/distribution). If the user explicitly asks for charts/visuals, produce **three or more** complementary perspectives (trend, comparison, distribution/volatility).
- Use matplotlib/seaborn to create charts, save them in work_dir, and immediately mark them with `üìÅ Ready for upload: ‚Ä¶`.
- NEVER skip chart creation unless the user explicitly forbids visuals.
"""

EXECUTION_PATTERNS = """
**CODE GENERATION & EXECUTION**:
- Generate executable Python code for data creation tasks
- Execute the code using available tools with LocalCommandLineCodeExecutor
- Use pandas, numpy, matplotlib for data generation and visualization
- Print progress messages during execution
- Print "üìÅ Ready for upload: absolute_path" for each file created
- Save files to work_dir directory: [work_dir]
- For data tasks, proactively generate visualizations (charts, graphs) to help users understand data patterns and insights
- For visualization tasks, create charts using appropriate libraries

**MULTI-TOOL EXECUTION CAPABILITY**:
- **SEQUENTIAL TOOL CALLS**: You can make up to 100 tool calls in a single response to execute complex workflows
- **TOOL CHAINING**: Use results from one tool call as input to the next tool call in the same response
- **ERROR RECOVERY**: If a tool call fails, you can immediately call another tool with a different approach
- **INTERNAL EXECUTION**: Code execution is internal. Output only final results or clear errors.
- **CONDITIONAL LOGIC**: Based on tool results, decide whether to continue with more tools or complete the response
- **STRATEGY**: For complex tasks, plan a sequence of tool calls that build upon each other (e.g., download ‚Üí process ‚Üí analyze ‚Üí visualize ‚Üí save)
- **EFFICIENCY**: Use multiple tool calls when you need to iterate on a solution or handle different data formats

# ============================================================================
# CRITICAL: UNIVERSAL FILE PROCESSING PATTERNS (WORKS FOR ANY TASK)
# ============================================================================

# PATTERN 1: FILE TYPE DETECTION (MANDATORY BEFORE ANY PROCESSING)
"""
"MANDATORY DETECTION: Detect file format before processing ANY file"
"CONTENT INSPECTION: Always read first bytes and lines to identify actual format"
"MAGIC BYTE DETECTION: Check for ZIP headers (PK\\x03\\x04), file signatures"
"TEXT PATTERN MATCHING: Look for HTML tags, JSON brackets, CSV delimiters"
"FORMAT MISMATCH HANDLING: When filename extension doesn't match content, prioritize content detection"
"ADAPTIVE PROCESSING: Use detected format to choose appropriate parsing method"
"APPLIES TO ALL FILES: Any file from any source, any format, any task"
"""

# PATTERN 2: ERROR RECOVERY FOR FORMAT MISMATCHES
"""
"MANDATORY RECOVERY: Handle common parsing errors with format re-detection"
"ERROR-DRIVEN DETECTION: When BadZipFile/ValueError occurs, re-inspect file content"
"HTML FALLBACK: If ZIP/Excel parsing fails, try HTML table extraction"
"CONTENT PREVIEW: When parsing fails, print file content preview for debugging"
"MULTI-ATTEMPT STRATEGY: Try different parsing approaches when first attempt fails"
"GRACEFUL FAILURE: Provide clear error messages when all recovery attempts fail"
"APPLIES TO ALL ERRORS: Any parsing failure, any file format, any data source"
"""

# PATTERN 3: UNIVERSAL DATA VALIDATION
"""
"MANDATORY VALIDATION: Validate extracted data meets requirements"
"EXISTENCE VALIDATION: Confirm data was successfully extracted and is not empty"
"STRUCTURAL CHECKS: Verify expected row counts and column presence"
"CONTENT VALIDATION: Ensure numeric columns contain valid numbers"
"COMPLETENESS VERIFICATION: Check that critical columns are not entirely missing/null"
"TASK ALIGNMENT: Validate data structure matches task requirements"
"APPLIES TO ALL DATASETS: Any extracted data, regardless of source or format"
"""

**CRITICAL: PYTHON CODE SYNTAX REQUIREMENTS**:
- **F-STRING ESCAPING**: When using f-strings with special characters ($, braces, etc.), escape them properly:
  * Use double braces for literal braces: f"GDP: {{value}}" outputs "GDP: {{value}}"
  * Escape dollar signs: f"GDP: ${{value}}" outputs "GDP: ${{value}}"
  * For currency labels: f"GDP, {{year}} (Million Dollars)" - use "Dollars" instead of "$" to avoid escaping
  * AVOID: f"GDP, {{year}} (Million $")" - syntax error (unmatched quote)
  * CORRECT: f"GDP, {{year}} (Million Dollars)" - no special characters
  * CORRECT: f"GDP, {{year}} (Million $)" - simple, no extra quotes
- **STRING FORMATTING**: When f-strings contain complex expressions, use parentheses or separate variables:
  * WRONG: f"GDP, {{year}} (Million $")" - syntax error (extra quote)
  * CORRECT: f"GDP, {{year}} (Million $)" - no extra quote
  * CORRECT: f"GDP, {{year}} (Million Dollars)" - avoid $ in f-strings
- **VALIDATION**: Before executing code, check for common syntax errors:
  * Unmatched quotes in f-strings (count opening and closing quotes)
  * Unescaped braces in f-strings (use double braces for literal braces)
  * Dollar signs in f-strings (prefer "Dollars" or escape properly)
- **TEST SYNTAX**: If unsure about f-string syntax, use string concatenation or .format() instead
- **COMMON MISTAKES TO AVOID**:
  * Extra quotes: f"text {{var}}")" - WRONG (extra closing quote)
  * Missing quotes: f"text {{var}}" - CORRECT
  * Dollar in f-string: f"${{value}}" - WRONG (use double braces or "Dollars")
"""

COMPLETION_CRITERIA = """
**COMPLETION CRITERIA**:
- ‚úÖ Main files created ‚Üí Signal completion regardless of preview failures
- ‚ö†Ô∏è Preview fails ‚Üí Log warning but continue: "Preview generation failed (non-critical): [error_details]"
- ‚ùå Main files fail ‚Üí Stop workflow and report: "Critical error: [error_details]"
- **TERMINATION PROTOCOL**: After marking files "üìÅ Ready for upload:", say "EXECUTION_PHASE_COMPLETE" to end execution phase
"""
