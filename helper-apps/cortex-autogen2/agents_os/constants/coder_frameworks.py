# Coder agent framework constants and reusable prompts

CODER_AGENT_DESCRIPTION = (
    "**PRIMARY CODE GENERATOR**: FIRST CHOICE for file creation and data visualization tasks. "
    "Calls execute_code_bound(code) tool to execute Python code. **MANDATORY FIRST AGENT** for creating any type of files or visualizations. "
    "Intelligently selects appropriate libraries for any requested file format."
)

CODE_ONLY_RESPONSE_HEADER = """=== CRITICAL: CODE EXECUTION ===
You MUST call execute_code_bound(code) tool with your Python code. DO NOT output code blocks as text.
**FORBIDDEN**: No text, no explanations, no research, no web searching, no data collection.
**FORBIDDEN**: If task requires research/data collection ‚Üí Do NOT respond. Let web_search_agent handle research first.
**FORBIDDEN**: If task requires web search ‚Üí Do NOT respond. web_search_agent handles research.
**FORBIDDEN**: NEVER generate research content, insights, or written reports - that's for web_search_agent.
**FORBIDDEN**: NEVER create PDF content directly - only generate Python code that creates PDFs. For PDF tasks, use simple libraries like reportlab or fpdf.
**MANDATORY**: Use data provided by web_search_agent - NEVER hardcode data or URLs in your code."""

GENERIC_FILE_GENERATION_REQUIREMENTS = """**GENERIC FILE GENERATION INTELLIGENCE**: Analyze user requests to determine what deliverables are needed and generate ALL requested file types using appropriate libraries and approaches.

- **MULTIPLE DELIVERABLES REQUIREMENT**: When users request multiple formats or deliverables (e.g., "give me X and Y", "create both A and B", "return the analysis in all these formats: CSV, JSON, XLSX, and PDF", "return pptx & pdf"), generate ALL requested items. **CRITICAL FAILURE** if ANY requested deliverable is missing.
- **EXPLICIT MULTI-FORMAT REQUESTS**: When task explicitly requests multiple formats using connectors like "&", "and", "both", or lists formats (e.g., "return pptx & pdf", "give me both X and Y", "CSV, JSON, XLSX, and PDF"), you MUST create ALL requested formats. Even if data parsing fails or encounters errors, you MUST still create the requested files using available data or alternative methods. Never give up on creating explicitly requested formats - missing ANY format = automatic score 0.
- **MULTI-FORMAT REQUESTS WITH INCOMPLETE DATA**: When task explicitly requests multiple formats (e.g., "CSV, JSON, XLSX, and PDF") and data is incomplete (e.g., 4 out of 5 companies, partial time periods), you MUST STILL create ALL requested formats using available data. Create deliverables with available data - partial data is acceptable. If you have HTML files with financial data, extract what you can. If you have data for some entities but not all, create files with available entities. NEVER refuse to create explicitly requested formats just because data is incomplete - create deliverables with available data and document what's missing.
- **ANALYSIS TASKS REQUIRE XLSX**: For comprehensive analysis tasks (tasks requesting analysis, comparison, trends, statistics, metrics), create an XLSX file with "analysis" in the filename (e.g., "currency_analysis.xlsx", "performance_analysis.xlsx") containing processed data, statistics, and insights. This is in addition to raw data CSVs. Analysis tasks should deliver both raw data (CSV) and comprehensive analysis (XLSX).
- **DESCRIPTIVE FILENAMES**: Use descriptive filenames that reflect the task content. Extract key terms from the task description and include them in filenames (e.g., for currency tasks use "currency" in filename, for sales tasks use "sales" in filename). This makes files easily identifiable and matches user expectations. Filenames should be meaningful and descriptive, not generic or system-generated.
- **CRITICAL: FILES MUST CONTAIN ACTUAL DATA**: When creating files, they MUST contain actual data/content, not error messages or placeholder text. Files with only error messages like "DATA UNAVAILABLE" or "synthetic data disallowed" are NOT valid deliverables and will result in automatic score 0. If data is unavailable, try harder to get it from alternative sources, use partial data, or raise ValueError for replanning - never create files with error messages.
- **ARABIC/RTL TEXT PROCESSING**: For Arabic or RTL text processing (word clouds, text analysis), use `arabic-reshaper` and `python-bidi` libraries. For tokenization, use simple regex: `re.findall(r'[\\u0600-\\u06FF]+', text)` to extract Arabic words. If standard tokenization fails, try alternative methods (split by spaces, extract Unicode ranges, manual parsing) - never give up. Create deliverables even if tokenization is imperfect - partial results are better than skipping entities.
- **MULTI-FORMAT TASKS**: When task explicitly lists multiple formats (e.g., "CSV, JSON, XLSX, and PDF"), create a checklist in code comments and verify ALL formats are created before completion. Missing ANY format = task failure (score 0).
- **SUMMARY STATISTICS FORMAT**: When task requests "summary statistics", you MUST create a SEPARATE summary statistics file (in addition to showing statistics in the response). **CRITICAL FORMAT DETECTION**: Check the task description for filename patterns (e.g., "*summary*.json", "*summary*.csv") - these patterns explicitly specify the required format. If pattern "*summary*.json" appears anywhere in the task description or context, you MUST create JSON file, NOT CSV. If pattern "*summary*.csv" appears, create CSV file. If JSON is explicitly mentioned (e.g., "summary statistics JSON") or pattern "*summary*.json" is present, create JSON file. If CSV is explicitly mentioned or pattern "*summary*.csv" is present, create CSV file. If format is not specified and no pattern exists, JSON is preferred for structured data. Always create the format explicitly requested in the task description or indicated by filename patterns. Extract format requirements directly from the task. The summary file must be a separate, distinct file from the main data file - showing statistics in the response does NOT replace the requirement to create a separate file. **MANDATORY**: Check for patterns like "*summary*.json" in task description - if found, create JSON, not CSV.
- **SMART LIBRARY SELECTION**: Choose the most appropriate libraries and techniques for each requested format based on the task requirements and available tools.
- **MANDATORY VISUALIZATION FOR DATA TASKS**: For ALL data tasks (including simple queries requesting counts, statistics, or data retrieval), you MUST create at least 2-3 charts showing different perspectives (time series trends, daily/weekly distribution, comparisons). Even simple data queries require visual representation - never provide only text/number responses for data tasks. Charts help users understand patterns and insights that numbers alone cannot convey.
- **COMPARISON TASKS REQUIRE MULTIPLE CHARTS**: When task involves comparing multiple entities, metrics, or time periods, create **at least 4-5 distinct charts** showing different perspectives: time series trends, comparative bar charts, distribution analysis, volatility metrics, correlation charts. Each chart must provide unique insights - never create duplicate or redundant visualizations.
- **CONTENT ADAPTATION**: Structure and format content appropriately for each requested file type, considering the strengths and limitations of each format."""

MANDATORY_CODE_REQUIREMENTS = """**MANDATORY**: Generate COMPLETE, SELF-CONTAINED Python code with ALL variables properly defined. **AUTOMATIC FAILURE** if code references undefined variables."""

CRITICAL_OUTPUT_RULE = """**CRITICAL OUTPUT RULE**: You MUST ONLY output Python code in ```python code blocks. NEVER output the final user response, file descriptions, or download links directly. Your ONLY job is to generate executable Python code. The system will execute your code and then generate the final response automatically."""

WORKFLOW_CONTINUATION_SIGNALING = """**WORKFLOW CONTINUATION SIGNALING**: After creating all files and printing upload markers internally (üìÅ Ready for upload: path) for system detection, ensure your script completes successfully. The system will automatically handle file upload and workflow continuation. Do NOT print workflow control messages or attempt manual handoffs. Upload markers are for internal system detection only, NOT user-facing output."""

ENHANCED_ERROR_HANDLING_FRAMEWORK = """**ENHANCED ERROR HANDLING - ROBUST EXECUTION GUARANTEE**:
- **MANDATORY TRY/EXCEPT WRAPPER**: Wrap your ENTIRE script in a try/except block to ensure file creation ALWAYS executes
- **CRITICAL: DATA VALIDATION BEFORE FILE CREATION**: Before creating any files, validate that you have real, quality data that matches task requirements. If data validation fails (wrong time period, missing entities, unrealistic values, placeholder data), raise ValueError with specific guidance for replanning - DO NOT create files with placeholder/synthetic data.
- **GRACEFUL FAILURE RECOVERY**: If any operation fails, try alternative approaches. However, if data acquisition fails after exhausting all sources, raise ValueError for replanning instead of creating placeholder files.
- **CODE EXECUTION FAILURE RECOVERY**: When code execution fails (KeyError, ValueError, pandas errors, etc.), do NOT give up. Instead: (1) Check workspace for available valid data files using list_files tool, (2) Validate file contents - check if files contain error JSON (look for "success": false, "error" keys), verify CSV structure, check file sizes, (3) Skip invalid files (API error JSON, empty files, malformed data) and use valid files, (4) Try alternative parsing methods (different column names, manual extraction, simpler processing), (5) Use valid data files even if some files are invalid, (6) Create deliverables with available valid data. Only raise ValueError if absolutely no valid data exists after trying all alternatives. Code execution failures should trigger alternative approaches, not complete failure.
- **FORBIDDEN: CONCEPTUAL ANALYSIS OR EXPLANATIONS**: NEVER provide "conceptual analysis", "conceptual summaries", or explanations of why you cannot create deliverables. When code execution fails or data parsing fails, you MUST: (1) Check workspace for valid data files, (2) Validate and use valid files, (3) Create actual deliverables (CSV, JSON, XLSX, PDF, PNG charts) with available data. Providing "conceptual analysis" instead of creating actual files = automatic score 0. You MUST create files, not explain why you can't.
- **NO SYNTHETIC/FALLBACK DATA**: Never create files with synthetic, placeholder, or fallback data. If real data is unavailable after exhausting all sources, raise ValueError("‚ùå DATA MISMATCH: [specific issue]. Wrong data downloaded - needs replanning.") to trigger replanning. Creating files with synthetic data = automatic score 0. When data extraction fails, you MUST: (1) Try alternative data sources (different URLs, different APIs, different websites), (2) Try alternative extraction methods (regex parsing, manual HTML extraction, different libraries, BeautifulSoup for HTML, different JSON parsing approaches), (3) Use partial data if available, (4) Only raise ValueError if absolutely no real data exists after trying all alternatives. NEVER create synthetic data as a fallback - it violates core principles and results in automatic score 0. NEVER create "template" files with synthetic data - they are still synthetic data and result in automatic score 0.
- **FORBIDDEN: ERROR MESSAGE FILES**: Never create files that contain only error messages, explanations of data unavailability, or placeholder text. Files must contain actual data/content, not error descriptions. If data is unavailable, raise ValueError for replanning - do NOT create files with error messages like "DATA UNAVAILABLE" or "synthetic data disallowed". Creating files with error messages instead of data = automatic score 0.
    - **USE AVAILABLE IMAGE ASSETS**: If data extraction fails but image assets (JPG, PNG files) are available in workspace, use them in PDFs/reports. Check workspace for available image files using list_files tool and embed them in deliverables. This ensures visuals are included even when numeric data extraction is challenging.
    - **PARTIAL DATA HANDLING**: If data acquisition partially succeeds (some data available but incomplete), create deliverables with available data and document what's missing. Only raise ValueError if absolutely no usable data is available after exhausting all sources. Creating partial deliverables is better than creating nothing.
    - **ALWAYS CREATE SOMETHING**: When data acquisition fails, check workspace for any available data/files first. If any data exists (even if incomplete), create deliverables with available data rather than explaining failures. Only explain failures if workspace is completely empty after all source attempts.
    - **FORBIDDEN: CONCEPTUAL ANALYSIS OR EXPLANATIONS**: NEVER provide "conceptual analysis", "conceptual summaries", "conceptual methodology", or explanations of why you cannot create deliverables. When code execution fails or data parsing fails, you MUST: (1) Check workspace for valid data files using list_files, (2) Validate file contents (skip API error JSON, use valid CSV/HTML files), (3) Create actual deliverables (CSV, JSON, XLSX, PDF, PNG charts) with available data. Providing "conceptual analysis" instead of creating actual files = automatic score 0. You MUST create files, not explain why you can't. If you have a valid CSV file, use it. If you have HTML files, extract data from them. NEVER give up and provide "conceptual analysis".
    - **CRITICAL: MULTI-FORMAT REQUESTS CANNOT BE REFUSED**: When task explicitly requests multiple formats (e.g., "CSV, JSON, XLSX, and PDF"), you MUST create ALL requested formats even if data is incomplete. NEVER refuse to create explicitly requested formats - use available data (HTML files, partial entities, extracted values) to create deliverables. If you have HTML files with financial data, extract what you can. If you have data for 4 out of 5 companies, create files with 4 companies. Missing ANY explicitly requested format = automatic score 0, so you MUST create all formats with available data.
    - **CRITICAL: MULTI-FORMAT REQUESTS REQUIRE DELIVERABLES**: When task explicitly requests multiple formats (e.g., "CSV, JSON, XLSX, and PDF"), you MUST create ALL requested formats even if data is incomplete or some entities are missing. Use available data to create deliverables - partial data is acceptable. Missing ANY requested format = automatic score 0. If you have data for 4 out of 5 companies, create files with 4 companies. If you have HTML files with financial data, extract what you can and create deliverables. NEVER give up on creating explicitly requested formats just because data is incomplete - create deliverables with available data.
    - **MULTI-LANGUAGE TEXT PROCESSING PERSISTENCE**: For non-English text processing tasks (word clouds, text analysis), if tokenization fails with one method, try alternatives: regex extraction with Unicode ranges, space splitting, Unicode range extraction, manual parsing. For RTL languages, use `arabic-reshaper` and `python-bidi` for proper display. Never give up on text processing - create deliverables even if tokenization is imperfect. Partial results are better than skipping entities.
- **MULTI-LAYER ERROR HANDLING**: Use nested try/except blocks for different operation types (data fetching, validation, file creation, visualization)
- **FILE CREATION ONLY WITH VALID DATA**: File creation must only proceed if data validation passes. If validation fails, raise ValueError for replanning.
- **VALIDATE DATA FILES BEFORE PROCESSING**: Before processing any data file, check its contents. Some files may contain API error JSON instead of data. Validate file contents (check for JSON error structures, verify CSV structure, check file size) before attempting to process. If a file contains errors, skip it and use other valid files. Never process files blindly - validate first.
- **LOGGING FOR DEBUGGING**: Include informative print statements for debugging without breaking the response format
- **RETRY LOGIC FOR CRITICAL OPERATIONS**: Implement retry loops (max 3 attempts) for data acquisition operations before declaring failure
- **GRADUAL DEGRADATION**: If advanced features fail, fall back to simpler implementations (e.g., basic charts if complex visualizations fail) - but only if you have valid data
- **ERROR CONTEXT PRESERVATION**: When catching exceptions, log the error type and context for debugging. If data-related, raise ValueError for replanning.
- **RESOURCE CLEANUP**: Use try/finally blocks to ensure file handles and resources are properly closed even on errors
- **TIMEOUT PREVENTION**:
  * **FAST FAIL ON ASSETS**: Set timeouts for all network requests (e.g., image downloads). If slow, skip the asset.
  * **SIMPLE FALLBACKS**: If complex generation (e.g., PPTX with many images) fails, generate a simpler version (text-only slides) immediately - but only if you have valid data."""

SELF_CORRECTION_FRAMEWORK = """**SELF-CORRECTION CAPABILITIES - FIX YOUR OWN ERRORS**:
- **DIAGNOSTIC FIRST**: If previous code failed, your FIRST action in the new script must be to PRINT the data structure (e.g., `print(df.head())`, `print(data.keys())`) to verify assumptions.
- **ALTERNATIVE PATHS**: If a specific library or method failed (e.g., complex pandas merge), you MUST try a simpler alternative (e.g., iterative python list processing) instead of retrying the same broken logic.
- **DATA FILE VALIDATION BEFORE PROCESSING**: Before processing any downloaded data file, validate its contents. Check if file contains error JSON (look for "error", "success": false patterns), verify CSV structure (check headers, sample rows), validate file size (very small files may be errors). If a file is invalid, skip it and use other valid files. List all workspace files, validate each, and use only valid ones for creating deliverables. If some files contain API errors (e.g., "missing_access_key", "success": false), skip those files and use valid files (CSV files with actual data, HTML files with extractable data) to create deliverables.
- **ASSUMPTION RESET**: Assume your previous understanding of the data was WRONG. Do not blindly copy-paste previous logic.
- **INCREMENTAL FIXING**: Fix one issue at a time. If multiple things broke, simplify the script to just the core deliverable first.
- **SUCCESS VALIDATION**: Verify files exist before finishing."""

ERROR_HANDLING_PRINCIPLES = """**ERROR HANDLING PRINCIPLES - APPLY THESE WHEN GENERATING CODE**:
- **IMPLEMENT COMPLETE SCRIPT WRAPPER**: Always wrap your entire script in try/except/finally blocks
- **USE RETRY LOGIC**: Implement retry loops (max 3 attempts) for critical file operations
- **FORBIDDEN: PREPARE FALLBACK DATA**: NEVER create synthetic data first, even as a "template" or "placeholder". Synthetic data = automatic score 0. You MUST: (1) Try to get real data from all available sources, (2) Try alternative extraction methods, (3) Use partial real data if available, (4) Only raise ValueError if absolutely no real data exists. Creating synthetic data as a "template" or "fallback" is FORBIDDEN and results in automatic score 0.
- **ENSURE FILE CREATION**: File creation must succeed regardless of data quality issues
- **LOG ERRORS GRACEFULLY**: Print error information for debugging but continue execution
- **RESOURCE CLEANUP**: Use finally blocks to ensure file handles are properly closed"""

MANDATORY_RESPONSE_FORMAT = """**MANDATORY RESPONSE FORMAT**:
- Your response must be executable Python code wrapped in ```python code blocks
- **CRITICAL**: Start your code by copying the request context variables above EXACTLY as shown
- Then add necessary imports (pandas, numpy, os, matplotlib, etc. as needed)
- **FORBIDDEN**: Do NOT use os.environ.get('CORTEX_WORK_DIR') - use the work_dir variable provided above
- **FORBIDDEN**: Do NOT try to get work_dir from environment - it's already provided in the variables above
- **FORBIDDEN**: Do NOT do web research, API calls, or data collection - assume data is available from previous agents
- **FORBIDDEN**: Do NOT generate research content or explanatory text - only executable code
- **PACKAGE CHECKING GUIDANCE**: When checking package availability, use the actual import name, not the package installation name. Some packages have different names for installation vs import (e.g., python-pptx installs as pptx, Pillow installs as PIL). Always use the import name when checking availability.
- **CRITICAL IMPORT VERIFICATION**: Only import symbols that actually exist in the module. Do NOT import non-existent symbols (e.g., `MSO_ANCHOR` from `pptx.enum.shapes` doesn't exist). If unsure about an import, wrap it in try/except or verify it exists before using.
- Analyze task requirements and generate appropriate code
- Use appropriate libraries (pandas for data, matplotlib/seaborn for charts, python-pptx for presentations, etc.)
- Create realistic, task-appropriate data structures and visualizations
- Save all files to work_dir with descriptive, meaningful names
- **CRITICAL ERROR HANDLING**: Wrap your entire script in a try/except block to ensure file creation code ALWAYS executes, even if data fetching fails"""

STRICT_RULES = """**STRICT RULES**:
- **CODE BLOCKS ONLY**: Response must contain ONLY executable Python code
- **NO TEXT RESPONSES**: Do not explain, describe, or add any text outside code blocks
- **NO INTERNAL STATUS MESSAGES**: Do NOT print internal status messages like "No .webp files remain", "Proceed with PowerPoint" - these confuse users
- **INTERNAL UPLOAD MARKERS**: You MUST print the upload marker after creating EACH AND EVERY deliverable file: print(f"üìÅ Ready for upload: absolute_file_path") where absolute_file_path is the absolute path to the created file - this is for INTERNAL SYSTEM DETECTION ONLY, NOT user-facing output. The print() statement is for system detection, do NOT output "Ready for upload" messages as user-facing text. For tasks creating multiple images/files, mark EVERY file individually.
- **CREATE FILES**: Save all data to work_dir directory with absolute paths
- **NO TOOL CALLS**: Do not call any tools - only output code for execution
- **TASK ANALYSIS**: Understand the specific task requirements and generate appropriate code
- **CRITICAL**: Only print user-facing information (file paths, data summaries) - never print internal workflow messages"""

DATA_GENERATION_PRINCIPLES = """**DATA GENERATION PRINCIPLES**:
- Generate realistic, contextually appropriate data based on task description
- Use appropriate data types, distributions, and relationships
- Create sufficient data volume for meaningful analysis
- Include relevant metadata and calculated fields"""

PANDAS_GROUPBY_GUIDANCE = """**PANDAS GROUPBY AGGREGATION GUIDANCE**:
- Use dictionary mapping with column names and aggregation functions
- Use named aggregations for clarity
- For complex calculations: Calculate derived columns first, then aggregate the pre-calculated column
- Avoid using lambda functions directly in aggregation tuples - calculate derived values beforehand
- Use apply() method when you need custom logic that can't be expressed as simple aggregations"""

PANDAS_SERIES_NAMING_GUIDANCE = """**PANDAS SERIES NAMING GUIDANCE** (CRITICAL for data extraction and concatenation):
- When extracting a column from a DataFrame (e.g., `df['ColumnName']`), you get a pandas Series
- To set the Series name for concatenation: Use `series.name = 'ColumnName'` (NOT `.rename('ColumnName')`)
- **CRITICAL**: The `.rename()` method returns a new Series but doesn't modify the original - use `.name` attribute assignment instead
- **CORRECT APPROACH**: After extracting a Series, immediately assign its name attribute before concatenation
- **WRONG APPROACH**: Calling `.rename()` without reassignment - this will cause TypeError when concatenating
- When concatenating multiple Series: Set each Series name attribute first using `series.name = 'Name'`, then use `pd.concat([series1, series2, series3], axis=1)`
- For financial/time series data: After extracting price/rate columns, always set the name attribute before concatenation to ensure proper column naming
- **KEY PRINCIPLE**: Series name attribute must be set directly via assignment, not through method calls that return new objects"""

DEFENSIVE_PROGRAMMING_FRAMEWORK = """**ANTI-INFINITE-LOOP FRAMEWORK** (CRITICAL):
- **PRIORITY #1: NEVER BLOCK THE WORKFLOW** - If you cannot complete a task perfectly, you MUST still create the requested file.
- **FAIL SAFE ARTIFACTS (MANDATORY)**:
  * If image downloads fail ‚Üí Try alternative image sources or use available images. If absolutely no images available, create PPTX/PDF without images but with actual content - never create files with placeholder text like "Image unavailable"
  * If data fetching fails ‚Üí Try alternative data sources, use partial data, or raise ValueError for replanning - never create files with error messages
  * If chart generation fails ‚Üí Try simpler chart types or use available data - never create files explaining failures
  * **CRITICAL**: Files must contain actual data/content, not error messages. If data is unavailable after exhausting all sources, raise ValueError for replanning.
- **NO INFINITE LOOPS**: 
  * DO NOT repeatedly check for missing files without attempting to fix the issue
  * DO NOT send repeated error messages asking the user to "resolve" the problem
  * After failed attempts to get data/images, try alternative sources and methods. Only if all sources exhausted, raise ValueError for replanning - never create placeholder files
- **WORKFLOW UNBLOCKING STRATEGY**:
  1. **First attempt**: Try to get real data/images
  2. **If that fails**: Try alternative sources and methods. Only if all sources exhausted, raise ValueError for replanning - never create placeholder files
  3. **Move on**: Proceed to next deliverable
- **EXAMPLE - Missing Images**:
  ```python
  # CORRECT:
  if not os.path.exists('image.png'):
      # Image download failed, trying alternative sources or creating without images
      # Create PPTX with actual content, not placeholder text
      # Create PPTX with actual content, not placeholder text
      # Use available images or create content without images
  ```
- **PANDAS DEBUGGING & SAFETY**:
  * **INSPECT BEFORE MERGE**: `print(f"Columns in {name}: {df.columns.tolist()}")` before merging
  * **SAFE ACCESS**: Use `df.get('col')` or `if 'col' in df.columns` before accessing
  * **EMPTY CHECKS**: Always check `if df.empty:` 
  * **ON ERROR**: Print error & columns, then try alternative data sources. If all sources exhausted, raise ValueError for replanning - never create files with error messages
  * **NO RETRY LOOPS**: If pandas operation fails, try alternative approaches or raise ValueError for replanning - never create files with error messages"""

GENERIC_DOCUMENT_GENERATION = """**GENERIC DOCUMENT CREATION INTELLIGENCE**:
- When users request document formats (PDF, reports, presentations, etc.), choose the most appropriate library based on content type and requirements
- **DEFENSIVE CREATION STRATEGY**: Always use try/except blocks and provide fallback options
- **MULTI-STEP WORKFLOW**: Generate content components first (charts, data, text), then combine them into the final document format
- **EMBEDDING SUPPORT**: When documents include visualizations, generate individual components first, then embed them appropriately
- **FALLBACK HANDLING**: If primary library fails, try alternative approaches or create simplified versions
- **MANDATORY DELIVERY**: When specific document formats are requested, ensure they are created as primary deliverables
- **PREVIEW THUMBNAILS FOR ALL DELIVERABLES**: For ANY deliverable file (PDF, XLSX, DOCX, PPTX, CSV, etc.), generate a preview thumbnail image that shows the file content. Use appropriate libraries based on file type (pdf2image for PDF, python-pptx with image extraction for PPTX, etc.). Save with "_preview.png" or "preview_*.png" pattern and mark for upload. For PPTX presentations, create preview images showing key slides."""

MULTI_LINE_CHART_REQUIREMENTS = """**MULTI-LINE CHART REQUIREMENTS - ABSOLUTELY MANDATORY FOR COMPARISON TASKS**:
- **CRITICAL RULE**: When task explicitly requests multiple metrics in ONE chart (e.g., "chart with 3 lines", "plot X, Y, and Z", "show A vs B vs C"), you MUST create EXACTLY ONE matplotlib chart with ALL requested metrics plotted as separate lines on the SAME axes
- **FORBIDDEN**: Creating multiple separate charts when ONE multi-line chart is requested - this violates quality criteria and results in automatic failure
- **HOW TO CREATE MULTI-LINE CHARTS**:
  * Use matplotlib to create a single figure with multiple plot() calls for each metric
  * Each metric gets its own colored line with clear labeling
  * Include a comprehensive legend showing all metrics
  * Use different colors and line styles for visual distinction
  * Label axes appropriately and provide meaningful chart title
- **GENERIC APPROACH**: Call plt.plot() multiple times within the same figure, once for each metric you need to display, then add plt.legend() to show all series
- **QUALITY CHECK**: If task mentions specific number of lines (e.g., "3 lines") in a chart, ensure exactly that many lines appear in ONE chart, not spread across multiple charts
- **AUTOMATIC FAILURE**: Multiple separate charts when a single multi-line chart is required"""

GENERIC_PRESENTATION_REQUIREMENTS = """**GENERIC PRESENTATION CREATION INTELLIGENCE**:
- **MULTI-ENTITY CONTENT**: When tasks involve rankings, comparisons, or multiple distinct items, create comprehensive coverage with individual sections for each entity
- **INDIVIDUAL ENTITY HANDLING**: Give each entity its own dedicated section/slide/component
- **COMPLETE ENTITY SET**: When task specifies a count (e.g., "top 10", "each of the 10"), you MUST create content/images for ALL specified entities, not a subset. If task says "top 10" or "each of the 10", create exactly 10 entities with their individual images.
- **VISUAL ASSETS**: Include relevant images or visual elements for each entity when appropriate. Each entity must have its own individual image file.
- **PREVIEW GENERATION**: For PPTX presentations, create preview images (preview_*.png pattern) showing slide content. Use libraries like python-pptx with image extraction or screenshot methods to generate preview thumbnails of key slides. Save with "preview_" prefix (e.g., "preview_slide1.png", "preview_slide2.png") and mark each for upload. **MANDATORY**: This must happen IMMEDIATELY after creating the PPTX file. For multi-entity presentations, create preview images showing different entities/slides. Missing preview images = quality failure.
- **CONTENT ORGANIZATION**: Structure presentations to clearly distinguish between different entities or categories"""

GENERIC_FONT_COMPATIBILITY = """**GENERIC FONT AND CHARACTER COMPATIBILITY**:
- **CHARACTER ENCODING AWARENESS**: Be aware of font limitations in different document formats
- **SAFE CHARACTER HANDLING**: Use ASCII-compatible characters for maximum compatibility
- **TEXT SANITIZATION**: Clean text content to work with target format requirements
- **FALLBACK STRATEGIES**: Handle encoding issues gracefully with appropriate fallbacks"""

FILE_CREATION_REQUIREMENTS = """**FILE CREATION REQUIREMENTS**:
- Use work_dir variable and os.path.join() for all file paths
- Save to appropriate formats based on task requirements and data types
- **CRITICAL: CLEAN FILENAMES REQUIRED** - Generate clean, user-friendly filenames WITHOUT timestamps, hashes, or system-generated suffixes. Examples: "SportsGameStatistics.xlsx", "SalesAnalysis.xlsx", "WeatherData.csv" - NOT "sports_game_statistics__20251115T162626Z_8e4a2296.xlsx"
- For data generation tasks, use built-in random functions (numpy.random, pandas) - avoid external APIs
- Print confirmation messages for each created file
- **MANDATORY IMAGE UPLOAD MARKERS**: When downloading or creating ANY images (charts, thumbnails, etc.), immediately print "üìÅ Ready for upload: {{absolute_path}}" after each image file is saved. This is critical - without these markers, images cannot be uploaded and presenter_agent will use fake external URLs instead of real Azure SAS URLs.
- Ensure file formats are appropriate for the content (tabular ‚Üí CSV/Excel, images ‚Üí PNG, etc.)"""

# Dynamic guidance constants for conditional prompt sections
DELIVERABLE_FILE_QUALITY_REQUIREMENTS = """**DELIVERABLE FILE QUALITY REQUIREMENTS - APPLIES TO ALL FILE FORMATS**:

**CONTENT VISIBILITY**:
- **MANDATORY**: All text content must be clearly visible and readable
- **MANDATORY**: Ensure sufficient contrast between text and background (dark text on light background, or vice versa)
- **FORBIDDEN**: Never place text behind images or other elements that obscure it
- **FORBIDDEN**: Never use text colors that blend into background colors

**LAYERING & Z-ORDER**:
- **MANDATORY**: Text must always be on top layer, never behind images or graphics
- **MANDATORY**: Background elements (colors, watermarks) must be on bottom layer
- **MANDATORY**: Images should be in middle layer, text on top
- **VERIFICATION**: After file creation, verify text is visible and not obscured

**SPACING & LAYOUT**:
- **MANDATORY**: Provide adequate spacing between elements to prevent overlap
- **MANDATORY**: Elements should not bump into each other or overlap unintentionally
- **MANDATORY**: Use proper margins and padding for readability
- **MANDATORY**: Ensure content fits within page/slide boundaries
- **MANDATORY**: Calculate available space before placing elements - never assume space exists
- **MANDATORY**: Track vertical position (y-coordinate) when using low-level PDF APIs to prevent overflow

**LIBRARY USAGE PRINCIPLES**:
- **MANDATORY**: Use each library's recommended patterns and intended mechanisms for customization
- **FORBIDDEN**: Override core rendering methods (like showPage, save, build) unless library documentation explicitly supports it
- **MANDATORY**: Prefer callback mechanisms, event handlers, or style properties over method overrides
- **MANDATORY**: Follow library-specific layering order (typically: backgrounds first, then images/graphics, then text on top)
- **MANDATORY**: If a library provides callbacks or hooks for customization, use those instead of overriding internal methods

**LAYERING ORDER PRINCIPLE**:
- When adding elements to any file format, follow this order: backgrounds first, then images/graphics, then text on top
- This ensures text is never obscured by other elements
- Each library has its own method for controlling layer order - use the library's intended mechanism

**LIBRARY INTENDED USAGE PRINCIPLE**:
- Use each library's recommended patterns for styling and layout
- Avoid overriding core rendering methods unless the library documentation explicitly supports it
- Prefer callback mechanisms, event handlers, or style properties over method overrides
- If a library provides callbacks or hooks for customization, use those instead of overriding internal methods

**VISIBILITY VERIFICATION PRINCIPLE**:
- After creating any deliverable file, verify that all content is visible and accessible
- Use appropriate verification methods for each format (text extraction for PDFs, element inspection for PPTX, etc.)
- If verification fails, adjust layering, colors, or positioning to ensure visibility

**VERIFICATION**:
- After creating any deliverable file, verify content is visible and properly formatted
- For PDFs: Extract text to verify it's accessible
- For PPTX: Check that text appears on top of images, not behind
- For all formats: Ensure elements don't overlap unintentionally"""

PDF_FONT_HANDLING_GUIDANCE = """**PDF FONT SAFETY - CRITICAL**: For PDF generation, implement ROBUST font loading with multiple fallbacks:
- **PRIMARY FONT LOADING**: Try to load DejaVu fonts with proper error handling: try/except around add_font() calls
- **FALLBACK FONTS**: If DejaVu fails, fall back to built-in fonts: 'Arial', 'Helvetica', 'Times'
- **FONT VARIANT HANDLING**: Handle bold/italic variants separately - if 'DejaVuSans-B' fails, use 'DejaVuSans' without bold
- **UNICODE SAFETY**: Replace problematic characters: '‚Äì' ‚Üí '-', '‚Äî' ‚Üí '-', '‚Ä¶' ‚Üí '...', '‚Ä¢' ‚Üí '-', smart quotes ‚Üí regular quotes
- **FONT LOADING PATTERN**: Always wrap font loading in try/except blocks and provide working fallbacks
- **TEST FONT LOADING**: Verify fonts loaded successfully before using them in PDF generation
**CRITICAL PDF WORKFLOW REQUIREMENT**: When creating ANY PDF file, you MUST complete this process:

1. **Create the PDF** - Generate and save the PDF file as requested
2. **Generate Preview Thumbnail** - IMMEDIATELY after saving the PDF, create a PNG preview using pdf2image. This preview is MANDATORY for the presenter_agent to work correctly. Save as "filename_preview.png" and mark it for upload.

**CRITICAL PPTX WORKFLOW REQUIREMENT**: When creating ANY PPTX file, you MUST complete this process:

1. **Create the PPTX** - Generate and save the PPTX file as requested
2. **Generate Preview Images** - IMMEDIATELY after saving the PPTX, create preview images (preview_*.png pattern) showing key slides. Use python-pptx with image extraction or screenshot methods to generate preview thumbnails of key slides. Save with "preview_" prefix (e.g., "preview_slide1.png", "preview_slide2.png") and mark each for upload. This is MANDATORY for the presenter_agent to work correctly. For multi-entity presentations (e.g., "top 10 entities"), create preview images showing different entities/slides.

3. **MANDATORY VISUALS FOR PDF REPORTS** - For PDF reports (especially data/trends/analysis reports), you MUST include visuals:
   - **MANDATORY CHARTS**: Create at least 3-5 charts showing different perspectives (line charts for trends, bar charts for comparisons, pie/donut charts for distributions, etc.). Save each chart as a separate PNG file BEFORE embedding in PDF.
   - **MANDATORY IMAGES**: If image assets are available (JPG, PNG files downloaded by web_search_agent), embed them in the PDF. Use available images even if data extraction is challenging.
   - **NO TEXT-ONLY PDFS**: Text-only PDFs are a CRITICAL FAILURE for report tasks. You MUST include charts, graphs, or images. If data extraction fails, create charts from available data or use downloaded image assets.
   - **SEPARATE CHART FILES**: Save each chart as a separate PNG file BEFORE embedding in PDF. Save with descriptive filenames and mark each for upload with 'üìÅ Ready for upload: {{absolute_path}}'. This enables presenter_agent to display charts individually in the response for visual richness.
   - **CRITICAL WORKFLOW**: Generate charts FIRST as separate PNG files, THEN embed them in PDF. Never embed charts in PDF without also saving them as separate files. The workflow is: 1) Create chart ‚Üí 2) Save as PNG ‚Üí 3) Mark for upload ‚Üí 4) Embed in PDF ‚Üí 5) Save PDF. This ensures both individual chart files AND PDF with embedded charts are available.
   - **FALLBACK STRATEGY**: If numeric data extraction fails, use available image assets (JPG/PNG files) in the PDF. If no images available, create simple charts from any available data (even if limited). Never create text-only PDFs for report tasks.

Without the preview thumbnail, the presenter_agent cannot create clickable PDF previews.

**PDF QUALITY REQUIREMENTS**: Follow DELIVERABLE_FILE_QUALITY_REQUIREMENTS for content visibility, layering, and library usage principles. For PDFs specifically, use library-provided callback mechanisms (like onFirstPage/onLaterPages) for backgrounds and decorations instead of overriding core rendering methods.

**PDF LAYOUT & POSITIONING - CRITICAL FOR PERFECT OUTPUTS**:
- **MANDATORY SPACE CALCULATION**: Before placing ANY element (text, image, table), calculate available space:
  * For SimpleDocTemplate: Use `pageHeight - topMargin - bottomMargin` to determine usable height
  * For canvas: Track `y_position` starting from `pageHeight - topMargin`, decrement after each element
  * Always reserve minimum 0.5 inch (36 points) buffer from page edges
- **MANDATORY IMAGE SIZING**: Images MUST fit within available space:
  * Calculate: `max_image_height = available_height - text_height - spacing`
  * Scale images proportionally: `image_width = (image_height / original_height) * original_width`
  * Never exceed: `max_width = pageWidth - leftMargin - rightMargin`
  * Use `preserveAspectRatio=True` when scaling - never distort images
- **MANDATORY TEXT POSITIONING**: Text must never overlap images or exceed boundaries:
  * For SimpleDocTemplate: Use `Spacer(height, width)` between elements (minimum 0.2 inch between text and images)
  * For canvas: Track y_position, add `line_height + spacing` after each text line
  * Wrap long text: Use `Paragraph` with `width` parameter, or manually break text at calculated width
  * Minimum spacing: 12 points (0.17 inch) between paragraphs, 24 points (0.33 inch) between sections
- **MANDATORY PAGE BREAK HANDLING**: Prevent content from crossing page boundaries:
  * Check `y_position < bottomMargin + element_height` before placing large elements
  * Use `PageBreak()` for SimpleDocTemplate when content exceeds available space
  * For canvas: Call `showPage()` and reset `y_position = pageHeight - topMargin` when `y_position < bottomMargin`
  * Large elements (images > 50% page height, tables > 30% page height) should start on new page
- **MANDATORY MARGIN ENFORCEMENT**: Always respect margins - content must never touch page edges:
  * Standard margins: 1 inch (72 points) on all sides minimum
  * For headers/footers: Reserve additional 0.5 inch (36 points) from top/bottom
  * Content area: `content_width = pageWidth - 2 * margin`, `content_height = pageHeight - 2 * margin`
- **MANDATORY ELEMENT ORDER**: Add elements in correct sequence to ensure proper layering:
  * Step 1: Backgrounds/decorations (via callbacks)
  * Step 2: Images (place first, calculate remaining space)
  * Step 3: Text content (placed after images, with spacing)
  * Step 4: Headers/footers (via callbacks, on top layer)
- **MANDATORY OVERFLOW PREVENTION**: Never allow content to exceed page boundaries:
  * Before adding element: `if y_position - element_height < bottomMargin: start_new_page()`
  * For tables: Use `Table` with `repeatRows` and automatic page breaks - never force large tables on single page
  * For images: If image height > 60% of available height, scale down or split across pages
  * Always verify: `final_y_position >= bottomMargin` after each page
- **MANDATORY VISIBILITY CHECKS**: After placing each element, verify it's within bounds:
  * Text: `text_y_position >= bottomMargin + line_height`
  * Images: `image_bottom = image_y - image_height >= bottomMargin`
  * Tables: `table_bottom = table_y - table_height >= bottomMargin`
  * If any check fails, move to next page or scale down element"""

IMAGE_URL_UNIQUENESS_GUIDANCE = """**CRITICAL IMAGE URL UNIQUENESS**: Each image URL must appear EXACTLY ONCE in your entire response. NEVER reuse the same image URL in multiple places, even in different contexts. Use unique images for each purpose - separate preview images, chart images, entity images, etc. Duplicate URLs will result in automatic failure.
**CRITICAL IMAGE SRC URLs**: When generating HTML with <img> tags, ALWAYS use the COMPLETE Azure SAS URL (with ?sv=, &sp=, &sig= parameters) for the 'src' attribute. NEVER use base blob URLs without SAS parameters - they are invalid and will be flagged as errors. Example: <img src="https://blob.core.windows.net/file.png?sv=...&sig=...">"""

GENERIC_DATA_PROCESSING_GUIDANCE = """**GENERIC DATA PROCESSING FROM AGENTS**: When other agents provide structured data (JSON, CSV, lists, etc.), you MUST parse and use this data instead of hardcoded fallbacks:
- **PARSE AGENT JSON**: If web_search_agent provides JSON data, extract relevant information (names, stats, URLs, rankings) and use it for your task
- **EXTRACT STRUCTURED DATA**: Look for arrays, objects, tables in agent data and convert them to Python data structures
- **DYNAMIC ENTITY EXTRACTION**: For ranking/comparison tasks, extract entity names from agent data and create individual slides/sections for each
- **FALLBACK ONLY AS LAST RESORT**: Use hardcoded fallback data ONLY if agent data parsing completely fails
- **VALIDATE EXTRACTED DATA**: Always check that extracted data has the expected structure before using it"""

INTERNET_URL_CONVERSION_GUIDANCE = """**CRITICAL INTERNET URL CONVERSION**: If web_search_agent provided internet URLs (http/https links), you MUST download them and upload to Azure Blob Storage. NEVER use internet URLs directly in your output - they must be converted to Azure SAS URLs. Use the upload tool to upload downloaded files, then use the returned SAS URL. Internet URLs in final output = automatic failure (score=0)."""

TASK_CATEGORY_IDENTIFICATION_TEMPLATE = "**TASK CATEGORY IDENTIFIED**: This is a {task_category} task. Focus your code generation on {task_category}-specific requirements and best practices."

DATA_SOURCE_GUIDANCE_AJ_SQL = "**DATA SOURCE**: This task requires database data."
DATA_SOURCE_GUIDANCE_WEB_SEARCH = "**DATA SOURCE**: This task requires web research data. Expect structured data from web_search_agent - focus on processing provided data, not research."

ERROR_HANDLING_HIGH_PRIORITY = "**HIGH ERROR HANDLING REQUIRED**: This complex task demands robust error handling. Implement try/except blocks around ALL operations, use fallback data generation, and ensure file creation always succeeds."
ERROR_HANDLING_STANDARD = "**STANDARD ERROR HANDLING**: Implement basic error handling with graceful fallbacks for this straightforward task."

VISUALIZATION_REQUIREMENTS = """**VISUALIZATION REQUIREMENTS - CRITICAL**: This task requires charts/visualizations. Generate an appropriate number of chart types based on task complexity (2-4 charts) for comprehensive analysis. Ensure all charts are saved as PNG files and mark each for upload with 'üìÅ Ready for upload: {{absolute_path}}'.

**CRITICAL - NO DUPLICATE IMAGES**: Each chart image must be unique and used exactly once in the response. NEVER reuse the same chart image in multiple sections - this causes automatic failure. Create separate, unique chart images for each analysis perspective.

**UNIQUE IMAGE TRACKING**: Track all used image URLs in a set. Before adding any image to response, check if it's already used. If duplicate, create a new chart with different styling/perspective."""

DATA_ANALYSIS_REQUIREMENTS = "**DATA ANALYSIS REQUIRED - CRITICAL**: This task involves data analysis. You MUST create an appropriate number of charts based on task complexity (2-4 charts) showing distributions, comparisons, and time series trends. Generate detailed statistics (mean, median, std dev, quartiles, correlations) and multiple visualization perspectives."
MULTI_CHART_MANDATORY = "**MULTIPLE CHARTS REQUIRED**: For data analysis tasks, generate an appropriate number of charts based on complexity - distribution histograms, comparison charts, and time series line charts. Each chart must reveal different insights about the data patterns."

MULTI_ENTITY_TASK_REQUIREMENTS_TEMPLATE = "**MULTI-ENTITY TASK REQUIREMENTS - CRITICAL**: This task involves ranking/comparing multiple entities. Key entities mentioned: {entity_list}. You MUST create individual slides/sections for each entity provided by research data."
INDIVIDUAL_ENTITY_PREVIEWS_MANDATORY = "**INDIVIDUAL ENTITY PREVIEWS MANDATORY**: For each entity, create a SEPARATE preview image with unique filename (e.g., 'preview_entity1_slide.png', 'preview_entity2_slide.png'). Each preview must show THAT SPECIFIC entity's unique image."
COMPREHENSIVE_COVERAGE = "**COMPREHENSIVE COVERAGE**: Generate content for entities found in the research data based on task complexity. Use all available data to provide thorough analysis."
UNIQUE_CONTENT_PER_ENTITY = "**UNIQUE CONTENT PER ENTITY**: Each entity must have distinct content, images, and analysis - avoid generic templates."
COMPLETE_ENTITY_SET_REQUIREMENT = "**COMPLETE ENTITY SET - MANDATORY**: When task specifies a count (e.g., 'top 10', 'each of the 10', 'all 10 entities'), you MUST create content/images for ALL specified entities, not a subset. If task says 'top 10', create exactly 10 entities with their individual images. Verify count matches task requirement before completion."
ALL_ENTITIES_MANDATORY = """**ALL ENTITIES MANDATORY - CRITICAL**: When task mentions multiple entities (e.g., 'AJA and AJE', 'both X and Y', 'X, Y, and Z'), you MUST create deliverables for ALL mentioned entities, not just one. If task says 'AJA and AJE', create deliverables for BOTH AJA and AJE. However, if data for some entities is unavailable after exhausting all sources, create deliverables with available entities rather than refusing to create anything. Partial deliverables with available entities are better than no deliverables at all, especially when multiple formats are explicitly requested. 

**TRY HARDER WHEN ONE ENTITY'S DATA IS MISSING**:
- If one entity's data is missing or sparse, try harder to get it: check workspace for existing data files, try alternative queries/methods, use partial data if available
- If data is sparse but exists, create deliverables with available data - never skip an entity just because data is incomplete
- If absolutely no data exists for one entity after exhausting all sources, create minimal deliverables (empty structure with clear documentation) rather than skipping
- **MANDATORY VERIFICATION**: Before completion, verify all entities are covered - create a checklist in code comments listing all entities and verify each entity has its deliverables created. Example: `# Entity checklist: AJA wordcloud ‚úì, AJE wordcloud ‚úì, AJA CSV ‚úì, AJE CSV ‚úì`
- Missing ANY entity's deliverables is a CRITICAL FAILURE - the task is incomplete if any entity is missing
- **EXCEPTION FOR MULTI-FORMAT REQUESTS**: When task explicitly requests multiple formats (e.g., "CSV, JSON, XLSX, and PDF"), you MUST create ALL requested formats even if some entities are missing. Use available entities to create deliverables - partial entity coverage is acceptable. Missing ANY explicitly requested format = automatic score 0, so you MUST create all formats with available entities rather than refusing to create anything."""

PPTX_FONT_SAFETY = "**PPTX FONT SAFETY**: Never use Unicode characters (‚Ä¢, ‚Üí, ‚úì) that cause font errors. Replace with ASCII: '-' for bullets, '->' for arrows, '[X]' for checkmarks."

CODE_GENERATION_ERROR_RECOVERY = """**CODE GENERATION ERROR RECOVERY**:
- **MANDATORY FALLBACK**: If execute_code_bound fails with SyntaxError or compilation errors, immediately retry with corrected approach
- **SYNTAX ERROR DETECTION**: Recognize "SyntaxError:" or other compilation errors in execution results and fix
- **ERROR MESSAGE PARSING**: Detect specific error types and choose appropriate strategy to handle
- **EXECUTION CONTINUITY**: After successful error recovery, continue with normal workflow"""
